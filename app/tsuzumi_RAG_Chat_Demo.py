import os
from dotenv import load_dotenv
import streamlit as st
from src.utils import *

from langchain_openai import AzureOpenAIEmbeddings
from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel

from langchain_core.messages import (
    HumanMessage, 
    AIMessage,
)

# .env èª­ã¿è¾¼ã¿
load_dotenv()

tsuzumi_model = os.getenv("AZURE_TSUZUMI_MODEL", "tsuzumi-7b-202509")
tsuzumi_version = os.getenv("AZURE_TSUZUMI_VERSION", "2024-05-01-preview")
embedding_model = os.getenv("AZURE_OPENAI_EMBEDDING_MODEL", "text-embedding-3-small")
embedding_version = os.getenv("AZURE_OPENAI_EMBEDDING_VERSION", "2024-12-01-preview")

USER_NAME = "user"
ASSISTANT_NAME = "assistant"

def main():

    #ã€€Title
    st.title('tsuzumi RAG Chat Demo')
    
    with st.sidebar:
        #APIè¨­å®š       
        st.subheader("ğŸ”‘ APIè¨­å®š")
        # Chatãƒ¢ãƒ‡ãƒ«ç”¨
        tmp_inference_endpoint = st.text_input("tsuzumiãƒ¢ãƒ‡ãƒ« Endpoint",
                                           value=st.session_state.get("AZURE_INFERENCE_ENDPOINT", "")
                                           )
        
        tmp_inference_cred = st.text_input("tsuzumiãƒ¢ãƒ‡ãƒ« API Key",
                                       type="password",
                                       value=st.session_state.get("AZURE_INFERENCE_CREDENTIAL", "")
                                       )
        
        # Embeddingç”¨
        tmp_openai_endpoint = st.text_input("embedding-3-smallãƒ¢ãƒ‡ãƒ« Endpoint",
                                        value=st.session_state.get("AZURE_OPENAI_ENDPOINT", "")
                                        )
        
        tmp_openai_key = st.text_input("embedding-3-smallãƒ¢ãƒ‡ãƒ« API Key",
                                   type="password",
                                   value=st.session_state.get("AZURE_OPENAI_API_KEY", "")
                                   )

        if st.button("ä¿å­˜ã—ã¦åæ˜ "):
            try:
                with st.spinner("æ¥ç¶šç¢ºèªä¸­..."):
                    # Chatãƒ¢ãƒ‡ãƒ«ã®æ¥ç¶šãƒ†ã‚¹ãƒˆ
                    test_model = AzureAIChatCompletionsModel(
                        azure_deployment=tsuzumi_model,
                        openai_api_version=tsuzumi_version,
                        endpoint=tmp_inference_endpoint,
                        credential=tmp_inference_cred,
                    )
                    _ = test_model.invoke("ping")
                    
                     # Embeddingã®æ¥ç¶šãƒ†ã‚¹ãƒˆ
                    test_embeddings = AzureOpenAIEmbeddings(
                        azure_deployment=embedding_model ,
                        openai_api_version=embedding_version ,
                        azure_endpoint=tmp_openai_endpoint,
                        api_key=tmp_openai_key,
                    )
                    _ = test_embeddings.embed_query("ping")

                # æˆåŠŸã—ãŸã‚‰ session_state ã«ä¿å­˜
                st.session_state["AZURE_INFERENCE_ENDPOINT"] = tmp_inference_endpoint
                st.session_state["AZURE_INFERENCE_CREDENTIAL"] = tmp_inference_cred
                st.session_state["AZURE_OPENAI_ENDPOINT"] = tmp_openai_endpoint
                st.session_state["AZURE_OPENAI_API_KEY"] = tmp_openai_key
                
                st.success("è¨­å®šã‚’åæ˜ ã—ã¾ã—ãŸ âœ…")

            except Exception as e:
                st.error(f"æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIè¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚: {e}")
                st.stop()
                
        # ä¿å­˜æ¸ˆã¿ã®å€¤ã‚’å–å¾—
        inference_endpoint = st.session_state.get("AZURE_INFERENCE_ENDPOINT")
        inference_cred = st.session_state.get("AZURE_INFERENCE_CREDENTIAL")
        openai_endpoint = st.session_state.get("AZURE_OPENAI_ENDPOINT")
        openai_key = st.session_state.get("AZURE_OPENAI_API_KEY")        
            
        st.markdown("<div style='margin:30px 0;'></div>", unsafe_allow_html=True)
        
        st.subheader("ğŸšï¸ é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢é–¾å€¤")
        score_threshold = st.slider(
            "ã‚¹ã‚³ã‚¢ã—ãã„å€¤", 
            min_value=0.0, 
            max_value=1.0, 
            value=0.3,   # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            step=0.05
        )
        
        st.subheader("ğŸ” æ¤œç´¢ä»¶æ•° (k)")
        k_value = st.slider(
            "æ¤œç´¢ä»¶æ•°", 
            min_value=1, 
            max_value=10, 
            value=3,    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            step=1
        )
        
        # ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã®å±¥æ­´ã‚’ä¿æŒã™ã‚‹æ•°
        st.subheader("ğŸ”¢ ä¼šè©±å±¥æ­´æ•°åˆ¶é™")
        message_num = st.slider('', min_value=5, max_value=50, value=10)
        
    # chatã®ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
    model = None
    if inference_endpoint and inference_cred:
        model = AzureAIChatCompletionsModel(
            azure_deployment="tsuzumi-7b-202508",
            openai_api_version="2024-05-01-preview",
            endpoint=inference_endpoint,
            credential=inference_cred,
        )
    else:
        st.warning("tsuzumiãƒ¢ãƒ‡ãƒ«ã®APIè¨­å®šã‚’ã—ã¦ãã ã•ã„")
        
    # embeddingsã®ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
    embeddings = None
    if openai_endpoint and openai_key:
        embeddings = AzureOpenAIEmbeddings(
            azure_deployment="embedding-3-small",
            openai_api_version="2024-12-01-preview",
            azure_endpoint=openai_endpoint,
            api_key=openai_key,
        )
    else:
        st.warning("embedding-3-smallãƒ¢ãƒ‡ãƒ«ã®APIè¨­å®šã‚’ã—ã¦ãã ã•ã„")
        st.stop()

    # Chainå–å¾—
    contextualize_chain = get_contextualize_prompt_chain(model)
    chain = get_chain(model)

    # FAISSã‹ã‚‰retrieverã‚’å–å¾—
    try:
        retriever = pull_from_faiss(embeddings, score_threshold=score_threshold, k=k_value)
    except FileNotFoundError:
        st.warning("ãƒ™ã‚¯ãƒˆãƒ«DBãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚å…ˆã«RAGç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    except Exception as e:
        st.error(f"ãƒ™ã‚¯ãƒˆãƒ«DBã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆè©³ç´°: {e}ï¼‰")
        st.stop()

    # ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’ä¿å­˜ã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’åˆæœŸåŒ–
    if "chat_log" not in st.session_state:
        st.session_state.chat_log = []

    # ãƒãƒ£ãƒƒãƒˆã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³    
    if st.button("ğŸ—‘ï¸", help="ãƒãƒ£ãƒƒãƒˆã‚’å‰Šé™¤"):
        st.session_state.chat_log = []
        
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ›   
    user_msg = st.chat_input("ã“ã“ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›")

    if user_msg:
        
        # ä»¥å‰ã®ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’è¡¨ç¤º
        for chat in st.session_state.chat_log:
            if isinstance(chat, AIMessage):
                with st.chat_message(ASSISTANT_NAME):
                    st.write(chat.content)
            else:
                with st.chat_message(USER_NAME):
                    st.write(chat.content)

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        with st.chat_message(USER_NAME):
            st.write(user_msg)
            
        # è³ªå•ã‚’ä¿®æ­£ã™ã‚‹
        if st.session_state.chat_log:
            new_msg = contextualize_chain.invoke({"chat_history": st.session_state.chat_log, "input": user_msg})
        else:
            new_msg = user_msg
        print(user_msg, "=>", new_msg)

        # é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        relavant_docs = retriever.invoke(new_msg, k=3)
        print("=== retriever raw docs ===")
        for i, d in enumerate(relavant_docs, 1):
            print(f"[{i}] type={type(d)}")
            if hasattr(d, "page_content"):
                print("content:", d.page_content[:200])
            else:
                print("no page_content:", d)

        # è³ªå•ã®å›ç­”ã‚’è¡¨ç¤º 
        response = ""
        with st.chat_message(ASSISTANT_NAME):
            msg_placeholder = st.empty()
            
            for r in chain.stream({"chat_history": st.session_state.chat_log, "context": relavant_docs, "input": user_msg}):
                response += r.content
                msg_placeholder.markdown(response + "â– ")
            msg_placeholder.markdown(response)
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚½ãƒ¼ã‚¹ã‚’è¡¨ç¤º
            if relavant_docs:
                sources = []
                seen = set()

                for doc in relavant_docs:
                    src = (doc.metadata or {}).get("source") 
                    if not src:
                        continue
                    if src in seen:
                        continue
                    seen.add(src)
                    sources.append(f'[[{len(sources)+1}]]({src})')

                if sources:  # â† ç©ºã§ãªã‘ã‚Œã°
                    st.markdown(" ".join(sources))

                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¡¨ç¤º
                cols = st.columns(len(relavant_docs))
                for i, doc in enumerate(relavant_docs):
                    with cols[i]:
                        with st.popover(f"å‚ç…§{i+1}"):
                            st.markdown(doc.page_content)    

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°(ä¼šè©±å±¥æ­´)ã‚’è¿½åŠ 
        st.session_state.chat_log.extend([
            HumanMessage(content=user_msg),
            AIMessage(content=response)
        ])
        
        # ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’ä¿æŒã™ã‚‹æ•°ã‚’è¶…ãˆãŸå ´åˆã€å¤ã„ãƒ­ã‚°ã‚’å‰Šé™¤
        if len(st.session_state.chat_log) > message_num:
            st.session_state.chat_log = st.session_state.chat_log[-message_num:]
        print(st.session_state.chat_log, len(st.session_state.chat_log))

if __name__ == '__main__':
    main()